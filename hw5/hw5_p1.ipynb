{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸš€ Open this Tutorial in Google Colab  \n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/amirhszd/MachineLearning4RemoteSensing/blob/main/hw5/hw5_p1.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "vLyW55aALgV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mount Google Drive and Load Data\n",
        "\n",
        "In this step, youâ€™ll mount your Google Drive so that you can access .npy files directly from it. Then, load your training, validation, and test sets using NumPy. Be sure to update the paths to point to your own data files.\n",
        "\n",
        "Make sure to set to runtime session to use GPU otherwise your training is going to be slow."
      ],
      "metadata": {
        "id": "6ltSaqOX_M_2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nup-H_t0DYyB",
        "outputId": "7a4f4a3e-8c1d-4b93-8cd8-3e22eff25cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'hw5'...\n",
            "fatal: repository 'https://github.com/amirhszd/MachineLearning4RemoteSensing/tree/main/hw5/' not found\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path/to/landis_chlorophyl_regression_train.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2595277045.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# TODO: Replace the paths below with the correct paths to your own files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path/to/landis_chlorophyl_regression_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path/to/landis_chlorophyl_regression_traingt.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/landis_chlorophyl_regression_train.npy'"
          ]
        }
      ],
      "source": [
        "# import files and mount google drive\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 1: Load your .npy files from your Google Drive\n",
        "# TODO: Replace the paths below with the correct paths to your own files\n",
        "\n",
        "X_train = np.load('C:/Users/jason/Desktop/git/MachineLearning4RemoteSensing/landis_chlorophyl_regression_train.npy')\n",
        "y_train = np.load('C:/Users/jason/Desktop/git/MachineLearning4RemoteSensing/landis_chlorophyl_regression_traingt.npy')\n",
        "\n",
        "X_val = np.load('C:/Users/jason/Desktop/git/MachineLearning4RemoteSensing/landis_chlorophyl_regression_val.npy')\n",
        "y_val = np.load('C:/Users/jason/Desktop/git/MachineLearning4RemoteSensing/landis_chlorophyl_regression_valgt.npy')\n",
        "\n",
        "X_test = np.load('C:/Users/jason/Desktop/git/MachineLearning4RemoteSensing/landis_chlorophyl_regression_test.npy')\n",
        "y_test = np.load('C:/Users/jason/Desktop/git/MachineLearning4RemoteSensing/landis_chlorophyl_regression_testgt.npy')\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXaHwV3zjS-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardize the Data & Create Dataset\n",
        "\n",
        "Here, you will standardize your dataset using StandardScaler. This ensures that each feature has zero mean and unit variance. Remember to fit the scaler only on the training data, and then transform all splits using that same scaler.\n",
        "\n",
        "Youâ€™ll define a custom PyTorch Dataset to wrap your data and make it compatible with DataLoader. After that, instantiate the datasets and set up data loaders with appropriate batch size and parallel loading settings (num_workers, prefetch_factor)."
      ],
      "metadata": {
        "id": "lCM6nnOw_U7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Standard scaling based on training data only\n",
        "# TODO: Scale your validation and test sets using statistics from the training set only\n",
        "X_train_scaled = # COMPLETE ME\n",
        "X_val_scaled = # COMPLETE ME\n",
        "X_test_scaled = # COMPLETE ME\n",
        "\n",
        "# Step 2: Create custom dataset class\n",
        "class ChlorophyllDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "      # TODO: Convert your numpy arrays to torch tensors with the correct dtype\n",
        "        self.X = # COMPLETE ME\n",
        "        self.y = # COMPLETE ME\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Step 3: Instantiate your datasets\n",
        "train_dataset = ChlorophyllDataset(X_train_scaled, y_train)\n",
        "val_dataset = ChlorophyllDataset(X_val_scaled, y_val)\n",
        "test_dataset = ChlorophyllDataset(X_test_scaled, y_test)\n",
        "\n",
        "# Step 4: Create DataLoaders\n",
        "# TODO: Set batch_size, num_workers, and prefetch_factor based on your system\n",
        "from torch.utils.data import DataLoader\n",
        "batch_size = # COMPLETE ME\n",
        "num_workers = # COMPLETE ME\n",
        "prefetch_factor = # COMPLETE ME\n",
        "train_loader = DataLoader(train_dataset, ...)# COMPLETE ME\n",
        "val_loader = DataLoader(val_dataset, ...)   # COMPLETE ME\n",
        "test_loader = DataLoader(test_dataset, ...) # COMPLETE ME\n",
        "\n",
        "print(\"train dataset size is:\", len(train_dataset))\n",
        "print(\"val dataset size is:\", len(val_dataset))\n",
        "print(\"test dataset size is:\", len(test_dataset))"
      ],
      "metadata": {
        "id": "X5fXXSWNGnpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Your 1D Convolutional Model\n",
        "In this step, youâ€™ll define a custom SpectralCNN model using PyTorchâ€™s nn Module. This model consists of a sequence of 1D convolutional layers with increasing filter sizes and stride to reduce the spectral dimension. The final Linear layer maps the learned features to a single output value for regression.\n",
        "\n",
        "ðŸ’¡ Note: Since your input has shape [batch_size, 425], youâ€™ll need to change the shape accordingly."
      ],
      "metadata": {
        "id": "u6IDq8GW_s6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class SpectralCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpectralCNN, self).__init__()\n",
        "        # 8 layer 1D CNN with kernel size 3 and stride of 2\n",
        "        # stride 2 lowers the dimensionality spectral dimensionality\n",
        "        # rely is the activation function introducing nonlinearity\n",
        "        # the bigger the kernel size the more number of parameters to learn\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(1, 8, kernel_size=3, stride=2), nn.ReLU(),  # [B, 8, 212]\n",
        "            nn.Conv1d(8, 16, kernel_size=3, stride=2), nn.ReLU(), # [B, 16, 105]\n",
        "            nn.Conv1d(16, 32, kernel_size=3, stride=2), nn.ReLU(),# [B, 32, 52]\n",
        "            nn.Conv1d(32, 64, kernel_size=3, stride=2), nn.ReLU(),# [B, 64, 25]\n",
        "            nn.Conv1d(64, 128, kernel_size=3, stride=2), nn.ReLU(),# [B, 128, 12]\n",
        "            nn.Conv1d(128, 128, kernel_size=3, stride=2), nn.ReLU(),# [B, 128, 5]\n",
        "            nn.Conv1d(128, 256, kernel_size=3, stride=2), nn.ReLU(),# [B, 256, 2]\n",
        "            nn.Conv1d(256, 256, kernel_size=2, stride=1), nn.ReLU(),# [B, 256, 1]\n",
        "            nn.Flatten(),\n",
        "            # we are performing regression so output is one value\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # TODO: x is of siz Nx425, however, we need to define the number of channels for our 1D data\n",
        "        # in this case there is only 1 channel/feature for our 425 input features\n",
        "        # in 2D space we have N x C_in x H x W.\n",
        "        # in 1D space we should have N x C_in x Length\n",
        "        # modify the retun correctly to reflect this\n",
        "        return # COMPLETE ME"
      ],
      "metadata": {
        "id": "k-ZeLhj7IAJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Model, Optimizer, and Training Configuration\n",
        "\n",
        "In this section, you will define the loss function for your regression task, instantiate your model, and set up the optimizer of your choice. Make sure to use weight decay to prevent overfitting.\n",
        "\n",
        "Your training will benefit from learning rate scheduler such as to gradually reduce the learning rate during training. Finally, youâ€™ll move your model to the GPU if available.\n",
        "\n",
        "ðŸ’¡ Tip: Your learning rate and weight decay are hyperparameters!"
      ],
      "metadata": {
        "id": "6MhICEWsOf3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define regression LOSS\n",
        "criterion = # COMPLETE ME\n",
        "\n",
        "# instantiating the model\n",
        "model = SpectralCNN()\n",
        "\n",
        "# TODO: define optimizer, suggestion is on Adam with weight decay.\n",
        "# keep in mind weight decay and learning rate are hyperpareters\n",
        "optimizer = # COMPLETE ME\n",
        "\n",
        "# TODO: you can define a learning rate scheduler at the time of training\n",
        "scheduler = # COMPLETE ME\n",
        "\n",
        "# Optional: use GPU if available\n",
        "# make sure to change your runtime type to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"Using:\", device)"
      ],
      "metadata": {
        "id": "g91cEqACI149"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop and Early Stopping\n",
        "\n",
        "In this section, you will implement the full training loop for your model. For each epoch, youâ€™ll:\n",
        "\tâ€¢\tLoop through your training data\n",
        "\tâ€¢\tSend batches to the correct device\n",
        "\tâ€¢\tPerform forward and backward passes\n",
        "\tâ€¢\tUpdate the model using your optimizer\n",
        "\n",
        "After training on each epoch, youâ€™ll evaluate your model on the validation set without computing gradients. Youâ€™ll track the best-performing model based on validation loss and save it using torch.save(). Finally, youâ€™ll step the learning rate scheduler to gradually reduce the learning rate over time.\n",
        "\n",
        "Make sure to plot loss for training and validation sets, this will help you see into overfitting and underfitting.\n",
        "\n",
        "ðŸ›‘ Donâ€™t forget: only update weights during training, and use torch.no_grad() during validation to save memory and speed up evaluation."
      ],
      "metadata": {
        "id": "5auafzgzAjQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize variables for early stopping\n",
        "best_val_loss = float(\"inf\")\n",
        "best_epoch = -1\n",
        "\n",
        "# keepong track of validation loss COMPLETE ME\n",
        "val_loss_history = []\n",
        "\n",
        "# Training loop\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    # TOOD: keep track of any other regression metric you want per\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        # TODO: send input features and reference target to device\n",
        "        X_batch, y_batch = # COMPLETE ME\n",
        "\n",
        "        # zeroing out previous step gradients\n",
        "        # COMPLETE ME\n",
        "\n",
        "        # send input features and reference target to device\n",
        "        outputs = model(X_batch)\n",
        "\n",
        "        # TODO: calculate the loss\n",
        "        loss = # COMPLETE ME\n",
        "\n",
        "        # TODO: calculate the gradients by calling backward on the loss\n",
        "        # COMPLETE ME\n",
        "\n",
        "        # TODO: take a step by calling step on the optimizer\n",
        "        # COMPLETE ME\n",
        "\n",
        "        # TODO: keep score of your training loss and any other metric you see fit for\n",
        "        # your regression task\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            # TODO: Same as training but we are not updating weights\n",
        "            # COMPLETE ME\n",
        "            # COMPLETE ME\n",
        "            # COMPLETE ME\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "    # TODO: implementing early stopping here.\n",
        "    # keep track of the validation loss and save the model parameters\n",
        "    # when the loss is the lowest for the validation set\n",
        "    val_loss_mean = # COMPLETE ME\n",
        "    val_loss_history....            # COMPLETE ME\n",
        "    if val_loss_mean < best_val_loss:\n",
        "        # COMPLETE ME\n",
        "        # COMPLETE ME\n",
        "        # COMPLETE ME\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "\n",
        "    # TODO: take a step in the scheduler to update the learning rate\n",
        "    scheduler...# COMPLETE ME\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {np.mean(train_losses):.4f} | Val Loss: {val_loss_mean:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "print(f\"\\nBest model saved from epoch {best_epoch+1} with Val Loss: {best_val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "ofJ-WIMV9-fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get predictions and plot your results\n",
        "Get predictions on your test set and plot your regression results for your training and test set."
      ],
      "metadata": {
        "id": "fmbe3TSIG2bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# TODO: we are to check the model's performance on the validation set\n",
        "# load the \"best_model.pt\" and pass your test data to it and get predictions\n",
        "# make sure the model is in eval mode, send the data to device\n",
        "# note: make sure you handled \"shuffle\" properly in your validation/testing set\n",
        "# loading the best model\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_model.pt\", weights_only=True))\n",
        "# COMPLETE ME\n",
        "# COMPLETE ME\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in val_loader:\n",
        "        # COMPLETE ME\n",
        "        # COMPLETE ME\n",
        "        # COMPLETE ME\n",
        "\n",
        "#TODO: plot regression plot and residual plots for your regression task,\n",
        "# report R2, and MAE along with any other metrics you want\n",
        "# analyze your results and report on your findings."
      ],
      "metadata": {
        "id": "9sGEEF6uRRk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am attaching the test results from Xgboost here for your reference! Your tuned model should outpefrom this performance:\n",
        "\n",
        "MAE=3.22, R2=0.96\n",
        "\n",
        "![xgboost](https://github.com/amirhszd/MachineLearning4RemoteSensing/blob/main/hw5/xgboost.png?raw=1)"
      ],
      "metadata": {
        "id": "K2DHbUVIKFpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparamter Tuning\n",
        "This part is all you. You have a base model that is performing somewhat accurately for you. Grab a package and perform hyperparameter tuning on your hyperparameters. Read Below!!!"
      ],
      "metadata": {
        "id": "MaXz1aWLHG7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: now that we have a base model that things are working, we perform hyperparameter tuning on validation data\n",
        "\n",
        "# change the learning rate value\n",
        "# number of layers\n",
        "# filter size (the bigger the filter size the more computationally expensive)\n",
        "# instead of ReLU use leaky Relu or Tanh\n",
        "# learning weight decay\n",
        "# using max/average pooling instead of stride 2\n",
        "# icorporating padding\n",
        "# you can log the lowest loss (highest accuracy) on your validation set and report that\n"
      ],
      "metadata": {
        "id": "zkEj1g1OXKD5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}